% Chapter Template

\chapter{Design} % Main chapter title

\label{Chapter3} % Change X to a consecutive number; for referencing this chapter elsewhere, use \ref{ChapterX}

\lhead{Chapter 3. \emph{Design}} % Change X to a consecutive number; this is for the header on each page - perhaps a shortened title

%----------------------------------------------------------------------------------------
%	SECTION 1
%----------------------------------------------------------------------------------------

\section{Introduction}

In order to answer the research question posed in Chapter 1, an experiment was designed. For convenience the research question is restated here:

``To what extent can an implementation of Defeasible Reasoning enhance the representation of a construct and support prediction capacity in comparison with Machine Learning?''

In order to answer this question an experiment demonstrating the ability of DR to enhance the representation of a construct and compare the prediction capacity of ML and DR was designed.


%----------------------------------------------------------------------------------------
%	EXPERIMENT DESIGN
%----------------------------------------------------------------------------------------

\section{Experiment Design}

\subsection{Representation of a Construct}
In order to answer the research question posed at the beginning of this dissertation the idea of a construct must be defined.
\cite{price2013research} discusses measurement in the context of psychology. He explains that many variables are simple to determine such as height, age, sex. We can gather this information by simply observing it. By contrast, a construct can be considered to be as a non-physical thing that cannot be simply measured.\footnote{For the purpose of this experiment, time is not a construct. Time can be measured with a watch.} 

Constructs are ideas people have about things, they may be idealised or may not really exist. Constructs are useful in science for improving our understanding of difference phenomena and techniques are often established in an attempt to quantify them. Examples of such constructs include intelligence (measured using IQ tests), personality (measured using surveys) and evolution (measured using changing characteristics over generations).

Representing constructs in a computable manner offers many potential benefits. By automating the representation of constructs we can free up the time of experts, for example, by automating diagnosis. We can reduce human error, for example in the case of reasoning. We can make predictions about certain situations and generally improve our understanding of phenomena. 

Developing systems that can represent constructs accurately is not trivial. There are a number of underlying issues that make solving this problem hard. Evaluating the performance of such systems is challenging as we have have no external measurement to compare the construct with. The same issues of implementation that occur in regular systems occur in these systems as well. There are a number of areas errors could arise:

\begin{itemize}

  \item Problems could occur as a result of errors in the data set
  \item the experts knowledge about the construct could be unsound
  \item the entire school of thought about the construct could be unsound
  \item the experts knowledge might be modeled incorrectly in the system
  \item the design of the system may be flawed drawing incorrect conclusions
  \item the implementation of the system may have underlying bugs that the software developer is unaware of.
  
\end{itemize}

In order to mitigate against these kinds of problems special care should be taken in gathering data for experiments. Experts should be selected carefully for modeling knowledge bases. There isn't much that the designer of a system can do to ensure that the knowledge of an entire field of study is correct. However, by interacting with a system for the evaluation of constructs, experts may gain a deeper understanding of the phenomena that they study. To ensure that an expert's knowledge base is modeled correctly user interfaces to the system should provide clear feedback and knowledge engineers should be trained to use them appropriately. Special care should be taken in the design of the system and appropriate test cases determined.

In order that a system can be evaluated using the techniques discussed in  section~\ref{sec:evaluation} the data set must contain some representation of the construct being studied. One method for evaluating the representation of the construct would be to have an expert manually determine labels for each row in a data set. These labels can then be compared with the output of an experiment to determine the accuracy of the output. 

This method is problematic as it is time consuming and may not yield accurate results. In order that a label is applied correctly an expert will need to study each row the data individually and take the time to assess it accurately. As this would need to be done for thousands of rows it will take up a lot of time. On top of this, the work is tedious so many experts will apply less scrutiny to each row resulting in less accurate labels.

Alternatively, the output can be compared with columns already present in the data but not used in the final result. It might be determined that there is a strong correlation between the construct and some other variable. If this variable is not included in the determination of the construct then we can measure the accuracy of our construct by seeing how it correlates to this variable.

\subsection{Implementing Construct Representation}

Constructs are typically made up of interleaved ideas and variables which makes their representation in computer systems complex. The structure of constructs varies and as a result different models may be developed to measure them. Similarly, different computational techniques may be used to represent these models.

For example, if we take intelligence as a construct, there are a number of approaches we could use to measure it. Intelligence can be represented by IQ; by computing the sum of an individuals scores on questions across a number of domains. If intelligence is modeled in this manner then it is easy to derive the model using linear regression. If on the other hand a more complex scoring system was used then a different supervised machine learning technique could be used. Similarly an expert could communicate with a group of people in a room and give their opinion on which people they believe are intelligent. This opinion then be used as labels for a data set in which a (very bad) model of intelligence could be made by training a classifier based on age, weight, sex etc. In another case, we could run clustering algorithms on out data. If the algorithm returns two clusters and one cluster has all the Nobel prize winners we might conclude that is the intelligent cluster. An expert could model intelligence defeasibly. For example, if someone performs well on tests they are intelligent but if they cheat on tests they are not. We could run semantics on those defeasible representations to determine who is intelligent.

None of the approaches outlined above can objectively say that they truly measure intelligence. However, if they are useful they will give some indication of how a person will perform on a task. The purpose of this example is to underline the differences in models and how these models can be represented using computational techniques.

It is with this in mind that an experiment is designed to compare these techniques. To do an experiment this has to be done:

\begin{itemize}

  \item A suitable construct must be chosen and access to an expert secured.
  \item Machine learning software must be procured and a number of machine learning techniques chosen for evaluation.
  \item A system based on defeasible reasoning must be designed.
  \item an experiment built on top of this must be chosen 
  
\end{itemize}

\subsection{Choice of Construct}

In order to perform the experiment a subject must be chosen. Access to an expert in the field of this subject needs to be secured as well as a data set that can be used to evaluate the representation of the construct. Suitable constructs don't have a conclusive easy to assess for of measurement such as  cancer, intelligence or personality. 

For this experiment Human Mental Workload was chosen as the construct. Mental Workload can loosely defined as the amount of effort it takes a user to perform a task. Researchers across a wide variety of domains study MWL, particularly those with interests in human performance and machine usability. \cite{meshkati2011human} outlines the problems that are associated with defining, quantifying and measuring MWL. Generalising MWL is difficult as it is a multifaceted phenomenon that varies depending on context. \cite{meshkati2011human} defines MWL as ``the operator's evaluation of the attentional load margin (between their motivated capacity and the current task demands) while achieving task performance in a mission-relevant context.'' Possible ways of measuring MWL include objectively (using task time, physiological indicators or task success) or subjectively (by having the participant answer introspective questions about their perceived state while completing the task).

One possible way of measuring MWL is using the NASA-Task Load Index (TLX). NASA-TLX is an introspective questionnaire in which participants subjectively assess their mental demand, physical demand, temporal demand, performance, effort and frustration after completing the task. Participants are then asked to compare the importance of these factors in their completion of the task. 

Another possible way to determine that MWL was high for a given task is based on the task completion time. If all variables are equal then task time can help us determine which tasks were most difficult.

Dr. Luca Longo from the DIT School of Computing is an expert in the area of MWL having completed his doctoral dissertation in formulating MWL as a defeasible construct. He has kindly volunteered his knowledge base to be used within the experiment. A data set was provided by Dr. Longo from previous research carried out in the field of web usability. The data set has the following columns (shown with example rows):

\begin{table}[]
\begin{center}
  \begin{tabular}{ | l | l | l | l | l | l | l | l | l | l | l | l | l | l | l | l | l | l | l | l | l | l | l | l | l |}
    \hline
expID & user & userID & taskID & time & mental & temporal & psychological & performance & effort & central & response & visual & auditory & spatial & verbal & manual & speech & arousal & bias & intention & knowledge & parallelism & skill & difficulty \hline \\
4 & imac1@gabi.com & 1 & 8 & 251 & 4 & 1 & 1 & 50 & 50 & 32 & 14 & 3 & 23 & 34 & 3 & 6 & 37 & 21 & 66 & 37 & 71 & 1 & 82 & 19.0 \hline \\
5 & imac1@gabi.com & 1 & 1 & 186 & 50 & 62 & 5 & 67 & 20 & 13 & 7 & 15 & 3 & 21 & 17 & 13 & 3 & 4 & 4 & 72 & 70 & 13 & 86 & 11.5 \hline \\
6 & imac2@gabi.com & 2 & 8 & 114 & 30 & 30 & 30 & 68 & 34 & 33 & 67 & 60 & 60 & 20 & 33 & 20 & 20 & 60 & 34 & 50 & 80 & 60 & 25 & 39.125 \hline \\
7 & imac2@gabi.com & 2 & 1 & 26 & 28 & 31 & 20 & 99 & 28 & 44 & 59 & 56 & 30 & 53 & 27 & 58 & 32 & 39 & 31 & 30 & 61 & 36 & 33 & 44.875 \hline \\
    \hline
  \end{tabular}
\end{center}
\caption{Caption}
\label{tab:my_label}
\end{table}

\citep{longo2014formalising} describes the rows in the dataset as follows:

\begin{table}[]
\centering
\begin{tabular}{|l|p{8cm}|}

\\ \hline
 expID & An ID that uniquely identifies the experiment instance \\ \hline
 user & The participant's email address \\ \hline
 userID & The participant's unique ID \\ \hline
 taskID & The experiment task ID undertaken by the participant \\ \hline
 time & The time taken by the participant to complete the task \\ \hline
 mental & The mental demand of the task, the answer to the question ``How much mental and perceptual activity was required (e.g., thinking, deciding, calculating, remembering, looking, searching, etc.)? Was the task easy (low mental demand) or complex (high mental demand)?'' on a scale from 0 - 100 \\ \hline
 temporal & The temporal demand, the answer to the question ``How much time pressure did you feel due to the rate or pace at which the tasks or task elements occurred? Was the pace slow and leisurely (low temporal demand) or rapid and frantic (high temporal demand)?'' on a scale from 0 - 100\\ \hline
 psychological & The effort expended completing the task, the answer to the question ``How much conscious mental effort or concentration was required? Was the task almost automatic (low effort) or it required total attention (high effort)?'' \\ \hline
 performance & The effort expended completing the task, the answer to the question ``How successful do you think you were in accomplishing the goal of the task? How satisfied were you with your performance in accomplishing the goal?'' on a scale from 0 - 100 \\ \hline
 effort & The effort expended completing the task, the answer to the question ``How much conscious mental effort or concentration was required? Was the task almost automatic (low effort) or it required total attention (high effort)?''on a scale from 0 - 100 \\ \hline
 central & The effort expended completing the task, the answer to the question ``'' on a scale from 0 - 100 \\ \hline
 response & The effort expended completing the task, the answer to the question ``'' on a scale from 0 - 100 \\ \hline
 visual & The effort expended completing the task, the answer to the question ``How much attention was required for executing the task based on the information visually received (through eyes)?'' on a scale from 0 - 100 \\ \hline
 auditory & The effort expended completing the task, the answer to the question ``How much attention was required for executing the task based on the information auditorily received (ears)?'' on a scale from 0 - 100 \\ \hline
 spatial & Spatial workload, the answer to the question ``How much attention was required for spatial processing (spatially pay attention around you)?'' on a scale from 0 - 100 \\ \hline
 verbal & Verbal workload, the answer to the question ``How much attention was required for verbal material (eg. reading or processing linguistic material or listening to verbal conversations)?'' on a scale from 0 - 100 \\ \hline
 manual & Manual effort, the answer to the question ``How much attention was required for manually respond to the task (eg. keyboard/mouse usage)?'' on a scale from 0 - 100 \\ \hline
 speech & The effort expended through speech, the answer to the question ``How much attention was required for producing the speech response(eg. engaging in a conversation or talk or answering questions)?'' on a scale from 0 - 100 \\ \hline
 arousal & The degree to which the participant was aroused, the answer to the question ``Were you aroused during the task? Were you sleepy, tired (low arousal) or fully awake and activated (high arousal)?'' on a scale from 0 - 100 \\ \hline
 bias & The context bias, ``How often interruptions on the task occurred? Were distractions (mobile, questions, noise, etc.) not important (low context bias) or did they influence your task (high context bias)?'' on a scale from 0 - 100 \\ \hline
 intention &  The effort expended completing the task, the answer to the question ``Were you motivated to complete the task?'' on a scale from 0 - 100 \\ \hline
 knowledge & The knowledge of the user before the experiment, the answer to the question ``How much experience do you have in performing the task or similar tasks on the same website?'' on a scale from 0 - 100 \\ \hline
 parallelism & The degree to which a user multi-tasked, the answer to the question ``Did you perform just this task (low parallelism) or were you doing other parallel tasks (high parallelism) (eg. multiple tabs/windows/programs)?'' on a scale from 0 - 100 \\ \hline
 skill & The effect of the participants skill level on completing the task, the answer to the question ``Did your skills have no influence (low) or did they help to execute the task (high)?'' on a scale from 0 - 100 \\ \hline
 difficulty & \( \frac{1}{8} \) ((solving/deciding) + (response) + (task/space) + (verbal material) + (visual resources) + (auditory resources) + (manual response) + (speech response)) \\ \hline
 
\end{tabular}
\caption{Caption}
\label{tab:my_label}
\end{table}


  \item A system based on defeasible reasoning must be designed.
  \item an experiment built on top of this must be chosen 

\subsection{Machine Learning Software}

In order to develop 





Develop software that allows an expert to model their knowledge base as a defeasible phenomenon. From this knowledge base the software will return a numeric value that is a representation of the construct that the expert is modeling. This numeric value is a representation of the expert's opinion of what the construct should be.
In the next phase of the experiment the same data set is used to evaluate machine learning. Machine learning's ability to represent a construct.


With respect to predictive capacity we are trying to answer the following question: ``Given data associated with an individual undertaking a task can we predict their mental workload.''








\subsection{Defeasible Reasoning Experiment}


1. DR Experiment: Input: an expert's knowledge base modeled with respect to a data set and a data set. Output: A numeric value that represents the construct being modeled.


Software is designed and implemented in order to elicit a knowledge base from an expert.
The expert uses the software to input their knowledge base.
The knowledge based is verified using same rows in the data set. It is examined in order to determine whether or not the knowledge base is free from error.
Once the knowledge base has been verified results can be determined. The results that are the value of the construct (MWL) and the degree of truth of the construct. These values are determined for the grounded semantic and for the preferred extension with the highest degree of truth. 

\subsection{Supervised Machine Learning Experiment}


2. ML Experiment: Input: the same data set as experiment 1.
Output: The results of machine learning algorithms after running on the data set. Error rates.

Comparison of predictive capacity.
Task time is often taken as an objective value when looking at mental work load.

The representation of an abstract construct as a regression problem. For many problems 





%----------------------------------------------------------------------------------------
%	SOFTWARE DESIGN
%----------------------------------------------------------------------------------------

\section{Software Design}

In order to demonstrate how DR can model a complex construct software was designed that would allow an expert to input their knowledge base as directed graph.
\subsection{Defeasible Knowledge Base}
The expert must be able to model their knowledge base as a defeasible process using a directed graph. One approach that could be taken in designing is to have the user input their knowledge base as a list of nodes and attack relations. The system designer specifies a format that the user conforms to and this can be then parsed by the software. An example of such a format inspired by JSON would be the following:

\begin{lstlisting}
"knowledge_base" {
    "arguments": [
                    "Low Effort->Low MWL", 
                    "High Effort->High MWL", 
                    "Low Performance->High MWL", 
                    "Low Effort & Low Performance->Low MWL"
                ],
    "attacks":  [
        ["Low Effort & Low Performance->Low MWL", "Low Performance->High MWL"]
    ]
}
\end{lstlisting}

This approach is simple to implement at the expense of a less usable system. For a trivial knowledge base with few arguments and attacks this approach is fine, however, once the knowledge base grows to the size approaching that of a real expert problems emerge.

It is time consuming and cumbersome for a user to have to type out an argument every time that they want to create an attack. It can be difficult for the user to keep track of the nodes and attacks. The format could be defined in such a way as to solve this problem, however, this would lead to a format that requires more parsing logic increasing the chances of errors and requiring the software to be more thoroughly tested.

For this reason the software has been designed to utilise a Graphical User Interface that allows a user to draw the directed graph. A user defines a node by clicking on an empty space on the graph. The user can name this node in order to keep track of what it represents. Once nodes have been created the user can then model the attack relations between the arguments by dragging from one node to another.

This approach has many advantages in comparison with the first approach. A non technical user will be more comfortable using a GUI than using the text based approach. When the knowledge base is input using text the user must take special care to ensure that the attack relations are correct.

\subsection{Membership Functions}

As the system is currently defined we will provide an expert with the ability to visualise their knowledge base in the form of a directed graph. As it stands the only information we have about an argument is it's attack relationship with other arguments and it's label (a natural language statement that allows the user to identify the argument and that may in some way describe the nature of the argument.) 

In order that the argumentation framework can be used to compute results a number of other concepts need to be designed into the system. The notion of whether or not an argument is activated or not needs to be modeled. Argument activation allows us to consider which attack relations to take into consideration for that tuple in the data set. We also need to generate a value that models our assessment of the construct based on the knowledge base.

In order to determine whether or not an argument is activated we will use fuzzy sets in a manner similar to Longo and Hederman. Fuzzy Sets were defined by Zadeh as ``a class of objects with a continuum of grades of membership.''\cite{zadeh1965fuzzy} These sets are characterised by membership functions; functions that take a value and map it to a number between 0 and 1 (where 0 indicates absence of the value in the set and 1 indicates it's presence.) This allows us to take a vague statement such as ``High Performance'' and determine to what extent a value of performance is considered to be high.

Arguments are based on one or more premises. Each premise corresponds to one column in the data set. By taking the value in this column the degree of truth of the premise as applied to the row can be determined using the membership function. We can determine the overall degree of truth for an argument by computing the average of the degrees of truth of the premises. The degree of truth for the argument is then used as the input for an argument output function contributes to the overall value associated with the construct.

If all associated values satisfy the membership functions of an argument, even with a very small degree of truth, then that argument is taken into consideration when evaluating the semantics of the AF for that row. If even one value associated with an arguments premise falls outside the membership function then the argument is disregarded.

The following is an example of this process. Taking the argument labeled ``Low Effort & Low Performance->Low MWL''; two premises can be identified: ``Low Effort'' and ``Low Performance'' and an output function ``Low MWL''. The premises and output function could be modeled as follows:




The most reliable way elicit membership and output functions from the user is to have them draw the functions by hand using the software. We can then use the data associated with this drawing to determine the appropriate output for a given tuple in the data set.

\subsection{Additional Software Requirements}

There are two additional requirements that need to be taken into consideration when allowing the user to develop an AF. The first is the concept of rebutting attacks. In order to model rebutting attacks the user must be able to draw edges in the graph with arrows on both ends. The second is modeling the concept of mitigating arguments. Some arguments weigh in on the final evaluation of the semantic without actually contributing to the value of the construct. These arguments must be taken into consideration but have their output ignored in the final value. 


There are additional requirements of the software that are standard features in many software systems and are provided here for the convenience of the user and to allow the experiment to proceed smoothly. These include allowing the user to save a knowledge base, load a previously generated knowledge base, load a data set and compute the results of applying the knowledge base to a row in the dataset.


\section{Conclusions}

This chapter presented the experiment undertaken to evaluate the hypothesis of this research project. The experiment will consist of a test of machine learning methods and defeasible reasoning. A software design is outlined order to undertake the defeasible reasoning portion of the evaluation. The design of the software includes the requirements for the user to be able to draw an argumentation framework that models their knowledge base and determine activation of the arguments within that framework by drawing membership functions.